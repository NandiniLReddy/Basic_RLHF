{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Simple RLHF Implementation by Nandini Lokesh Reddy"
      ],
      "metadata": {
        "id": "Zb6m9PIJ59PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RLHF Implementation for Text Summarization\n",
        "# First, install required packages\n",
        "!pip install -q transformers==4.28.1 datasets==2.12.0 torch==2.0.0 numpy tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7gWOmKIs-CD",
        "outputId": "df0c2e3c-e801-4ef4-fbfa-f8f1fc3c4025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "FieOMgrG54o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        "    AdamW\n",
        ")\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create sample data if needed\n",
        "def create_sample_data():\n",
        "    \"\"\"Create sample data files for testing.\"\"\"\n",
        "    sample_data = [\n",
        "        {\n",
        "            \"input_text\": \"I live right next to a huge university, and have been applying for a variety of jobs with them through their faceless electronic jobs portal for a few months.\",\n",
        "            \"candidate_0\": \"When applying through a massive job portal, is just one HR person seeing ALL of them?\",\n",
        "            \"candidate_1\": \"When applying to many jobs through a single university jobs portal, is just one HR person reading ALL my applications?\",\n",
        "            \"choice\": 1\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"I currently live in Texas and I plan on going to university in England, and I think I want to stay there for a while.\",\n",
        "            \"candidate_0\": \"I want to go on a road trip from Texas to England to visit as many places as possible. Which route should I choose?\",\n",
        "            \"candidate_1\": \"How do I plan a road trip in a way that I can see the places I want to see, but also see the places I haven't seen?\",\n",
        "            \"choice\": 1\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if not os.path.exists('sample_preference.jsonl'):\n",
        "        with open('sample_preference.jsonl', 'w') as f:\n",
        "            for item in sample_data:\n",
        "                f.write(json.dumps(item) + '\\n')\n",
        "        print(\"Created sample_preference.jsonl\")\n",
        "\n",
        "    if not os.path.exists('sample_prompt.jsonl'):\n",
        "        with open('sample_prompt.jsonl', 'w') as f:\n",
        "            for item in sample_data[:1]:  # Just add one item\n",
        "                f.write(json.dumps(item) + '\\n')\n",
        "        print(\"Created sample_prompt.jsonl\")\n",
        "\n",
        "create_sample_data()\n",
        "\n",
        "# Step 1: Custom Dataset for Preference Data\n",
        "class PreferenceDataset(Dataset):\n",
        "    def __init__(self, file_paths, tokenizer, max_length=512):\n",
        "        self.examples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Load data from JSONL files\n",
        "        for file_path in file_paths:\n",
        "            if os.path.exists(file_path):\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    for line in f:\n",
        "                        if line.strip():\n",
        "                            try:\n",
        "                                item = json.loads(line)\n",
        "                                if 'input_text' in item and 'candidate_0' in item and 'candidate_1' in item and 'choice' in item:\n",
        "                                    self.examples.append(item)\n",
        "                            except json.JSONDecodeError:\n",
        "                                print(f\"Skipping invalid JSON line in {file_path}\")\n",
        "            else:\n",
        "                print(f\"Warning: File {file_path} not found\")\n",
        "\n",
        "        print(f\"Loaded {len(self.examples)} examples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.examples[idx]\n",
        "        prompt = item['input_text']\n",
        "        chosen_idx = item['choice']\n",
        "        rejected_idx = 1 - chosen_idx\n",
        "\n",
        "        # Get chosen and rejected summaries\n",
        "        chosen = item[f'candidate_{chosen_idx}']\n",
        "        rejected = item[f'candidate_{rejected_idx}']\n",
        "\n",
        "        # Prepare inputs for reward model training\n",
        "        chosen_text = f\"Prompt: {prompt}\\nSummary: {chosen}\"\n",
        "        rejected_text = f\"Prompt: {prompt}\\nSummary: {rejected}\"\n",
        "\n",
        "        # Tokenize\n",
        "        chosen_encodings = self.tokenizer(\n",
        "            chosen_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        rejected_encodings = self.tokenizer(\n",
        "            rejected_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Prepare inputs for policy model\n",
        "        prompt_text = f\"Write a concise summary of the following text:\\n{prompt}\\nSummary:\"\n",
        "        prompt_encodings = self.tokenizer(\n",
        "            prompt_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"prompt\": prompt,\n",
        "            \"chosen\": chosen,\n",
        "            \"rejected\": rejected,\n",
        "            \"input_ids_chosen\": chosen_encodings[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask_chosen\": chosen_encodings[\"attention_mask\"].squeeze(),\n",
        "            \"input_ids_rejected\": rejected_encodings[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask_rejected\": rejected_encodings[\"attention_mask\"].squeeze(),\n",
        "            \"input_ids_prompt\": prompt_encodings[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask_prompt\": prompt_encodings[\"attention_mask\"].squeeze(),\n",
        "        }\n",
        "\n",
        "# Step 2: Define a simple Reward Model\n",
        "class SimpleRewardModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleRewardModel, self).__init__()\n",
        "        try:\n",
        "            # Try to load a pre-trained model\n",
        "            self.model = RobertaForSequenceClassification.from_pretrained(\n",
        "                \"roberta-base\",\n",
        "                num_labels=1\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading pre-trained model: {e}\")\n",
        "            # Fallback to a simple model\n",
        "            print(\"Using a simplified model instead\")\n",
        "            self.encoder = torch.nn.Embedding(50265, 768)  # RoBERTa vocab size\n",
        "            self.lstm = torch.nn.LSTM(768, 768, batch_first=True)\n",
        "            self.classifier = torch.nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        try:\n",
        "            # Try using the pre-trained model\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            return outputs.logits\n",
        "        except AttributeError:\n",
        "            # Fallback to the simple model\n",
        "            embeddings = self.encoder(input_ids)\n",
        "            packed_output, (hidden, _) = self.lstm(embeddings)\n",
        "            logits = self.classifier(hidden.squeeze(0))\n",
        "            return logits\n",
        "\n",
        "# Step 3: Train the Reward Model\n",
        "def train_reward_model(reward_model, dataset, epochs=3, batch_size=4, learning_rate=1e-5):\n",
        "    \"\"\"Train the reward model on preference data.\"\"\"\n",
        "    # Move model to device\n",
        "    reward_model.to(device)\n",
        "\n",
        "    # Create data loader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = AdamW(reward_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    reward_model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            # Get inputs\n",
        "            chosen_input_ids = batch[\"input_ids_chosen\"].to(device)\n",
        "            chosen_attention_mask = batch[\"attention_mask_chosen\"].to(device)\n",
        "            rejected_input_ids = batch[\"input_ids_rejected\"].to(device)\n",
        "            rejected_attention_mask = batch[\"attention_mask_rejected\"].to(device)\n",
        "\n",
        "            # Get reward scores\n",
        "            chosen_rewards = reward_model(chosen_input_ids, chosen_attention_mask)\n",
        "            rejected_rewards = reward_model(rejected_input_ids, rejected_attention_mask)\n",
        "\n",
        "            # Compute loss (chosen should have higher reward than rejected)\n",
        "            loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards) + 1e-8).mean()\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return reward_model\n",
        "\n",
        "# Step 4: Define a simple version of the PPO algorithm\n",
        "class SimplePPOTrainer:\n",
        "    def __init__(self, policy_model, reward_model, tokenizer,\n",
        "                 learning_rate=1e-5, epsilon=0.2):\n",
        "        \"\"\"\n",
        "        Initialize a simple PPO trainer for RLHF.\n",
        "        \"\"\"\n",
        "        self.policy_model = policy_model\n",
        "        self.reward_model = reward_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.optimizer = AdamW(policy_model.parameters(), lr=learning_rate)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def generate_responses(self, input_ids, attention_mask, max_new_tokens=50):\n",
        "        \"\"\"Generate responses from the policy model.\"\"\"\n",
        "        self.policy_model.eval()\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                # Generate output\n",
        "                outputs = self.policy_model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                    top_k=50,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            # Get generated tokens (excluding input)\n",
        "            # Handle the case where input_ids might be padded differently\n",
        "            response_ids = []\n",
        "            responses = []\n",
        "\n",
        "            for i, generated in enumerate(outputs):\n",
        "                # Find the actual length of this input\n",
        "                input_len = attention_mask[i].sum().item()\n",
        "\n",
        "                # Extract only the newly generated part\n",
        "                response = generated[input_len:]\n",
        "\n",
        "                # Convert to text\n",
        "                text = self.tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "                response_ids.append(response)\n",
        "                responses.append(text)\n",
        "\n",
        "            return responses, response_ids\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in generation: {e}\")\n",
        "            # Fallback: return empty responses\n",
        "            batch_size = input_ids.size(0)\n",
        "            empty_response = torch.tensor([[self.tokenizer.eos_token_id]], device=device)\n",
        "            empty_responses = [empty_response] * batch_size\n",
        "            empty_texts = [\"\"] * batch_size\n",
        "            return empty_texts, empty_responses\n",
        "\n",
        "    def compute_rewards(self, prompts, responses):\n",
        "        \"\"\"Compute rewards for generated responses.\"\"\"\n",
        "        try:\n",
        "            texts = [f\"Prompt: {prompt}\\nSummary: {response}\"\n",
        "                    for prompt, response in zip(prompts, responses)]\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Get rewards\n",
        "            with torch.no_grad():\n",
        "                rewards = self.reward_model(**inputs)\n",
        "\n",
        "            # Handle different possible return shapes\n",
        "            if hasattr(rewards, 'logits'):\n",
        "                rewards = rewards.logits\n",
        "\n",
        "            # Ensure we have the right shape\n",
        "            if rewards.dim() == 2 and rewards.size(1) == 1:\n",
        "                rewards = rewards.squeeze(1)\n",
        "\n",
        "            return rewards.squeeze()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing rewards: {e}\")\n",
        "            # Return neutral rewards\n",
        "            return torch.zeros(len(prompts), device=device)\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Perform a single PPO training step.\"\"\"\n",
        "        # Get inputs\n",
        "        input_ids = batch[\"input_ids_prompt\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask_prompt\"].to(device)\n",
        "        prompts = batch[\"prompt\"]\n",
        "\n",
        "        # Generate responses\n",
        "        responses, response_ids = self.generate_responses(input_ids, attention_mask)\n",
        "\n",
        "        # Compute rewards\n",
        "        rewards = self.compute_rewards(prompts, responses)\n",
        "\n",
        "        # Compute advantages (simplified)\n",
        "        baseline = rewards.mean()\n",
        "        advantages = rewards - baseline\n",
        "\n",
        "        # Prepare inputs for policy model - Fixed tensor alignment issue\n",
        "        # Instead of using response_ids directly as labels, we create a combined input\n",
        "        # and compute loss using a causal language modeling approach\n",
        "\n",
        "        # Create combined input_ids with response (shifted for causal LM training)\n",
        "        combined_ids = []\n",
        "        for inp, resp in zip(input_ids, response_ids):\n",
        "            # Concatenate input and response, limited to avoid exceeding sequence length\n",
        "            combined = torch.cat([inp, resp], dim=0)\n",
        "            combined_ids.append(combined)\n",
        "\n",
        "        # Pad to the same length\n",
        "        max_len = max(len(ids) for ids in combined_ids)\n",
        "        padded_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        for ids in combined_ids:\n",
        "            # Pad with attention mask\n",
        "            if len(ids) < max_len:\n",
        "                padding = torch.ones(max_len - len(ids), dtype=torch.long, device=device) * self.tokenizer.eos_token_id\n",
        "                padded = torch.cat([ids, padding], dim=0)\n",
        "                mask = torch.cat([torch.ones(len(ids), device=device),\n",
        "                                 torch.zeros(max_len - len(ids), device=device)], dim=0)\n",
        "            else:\n",
        "                padded = ids[:max_len]\n",
        "                mask = torch.ones(max_len, device=device)\n",
        "\n",
        "            padded_ids.append(padded)\n",
        "            attention_masks.append(mask)\n",
        "\n",
        "        # Stack to create batch\n",
        "        stacked_ids = torch.stack(padded_ids)\n",
        "        stacked_masks = torch.stack(attention_masks)\n",
        "\n",
        "        # Forward pass with shifted labels for causal LM\n",
        "        self.policy_model.train()\n",
        "\n",
        "        # Create labels by shifting the input right (standard causal LM approach)\n",
        "        labels = stacked_ids.clone()\n",
        "        labels[:, :-1] = stacked_ids[:, 1:]  # Shift right\n",
        "        labels[:, -1] = self.tokenizer.eos_token_id  # Last token predicts EOS\n",
        "\n",
        "        # Zero out labels for input part - we only want to compute loss on the response\n",
        "        input_lengths = [len(inp) for inp in input_ids]\n",
        "        for i, length in enumerate(input_lengths):\n",
        "            labels[i, :length-1] = -100  # Ignore these tokens in loss calculation\n",
        "\n",
        "        # Forward pass with properly aligned tensors\n",
        "        outputs = self.policy_model(\n",
        "            input_ids=stacked_ids,\n",
        "            attention_mask=stacked_masks,\n",
        "            labels=labels\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Scale loss by advantages to implement a simple form of PPO\n",
        "        ppo_loss = loss * advantages.mean()\n",
        "\n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        ppo_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return ppo_loss.item(), rewards.mean().item()\n",
        "\n",
        "    def train(self, dataloader, epochs=2):\n",
        "        \"\"\"Train the policy model.\"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            total_reward = 0\n",
        "\n",
        "            for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "                loss, reward = self.train_step(batch)\n",
        "                total_loss += loss\n",
        "                total_reward += reward\n",
        "\n",
        "            avg_loss = total_loss / len(dataloader)\n",
        "            avg_reward = total_reward / len(dataloader)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}, Avg Reward: {avg_reward:.4f}\")\n",
        "\n",
        "# Step 5: Main RLHF Pipeline\n",
        "def run_simplified_rlhf(file_paths):\n",
        "    \"\"\"Run a simplified RLHF pipeline that should work in most environments.\"\"\"\n",
        "    # Load tokenizers directly to avoid dependency issues\n",
        "    try:\n",
        "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "        print(\"Loaded RoBERTa tokenizer\")\n",
        "    except:\n",
        "        # Fallback to GPT2 tokenizer\n",
        "        try:\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            print(\"Loaded GPT2 tokenizer\")\n",
        "        except:\n",
        "            # Emergency fallback - create a minimal tokenizer\n",
        "            print(\"Failed to load pre-trained tokenizers, using minimal tokenizer\")\n",
        "            from transformers import PreTrainedTokenizer\n",
        "\n",
        "            class MinimalTokenizer(PreTrainedTokenizer):\n",
        "                def __init__(self):\n",
        "                    super().__init__()\n",
        "                    # Simple vocab: special tokens + ASCII\n",
        "                    self.vocab = {\n",
        "                        \"<pad>\": 0,\n",
        "                        \"<eos>\": 1,\n",
        "                        \"<unk>\": 2\n",
        "                    }\n",
        "                    # Add ASCII characters\n",
        "                    for i in range(128):\n",
        "                        char = chr(i)\n",
        "                        if char not in self.vocab:\n",
        "                            self.vocab[char] = len(self.vocab)\n",
        "\n",
        "                    self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n",
        "                    self.pad_token = \"<pad>\"\n",
        "                    self.eos_token = \"<eos>\"\n",
        "                    self.unk_token = \"<unk>\"\n",
        "                    self.pad_token_id = 0\n",
        "                    self.eos_token_id = 1\n",
        "                    self.unk_token_id = 2\n",
        "\n",
        "                def _tokenize(self, text):\n",
        "                    return list(text)\n",
        "\n",
        "                def _convert_token_to_id(self, token):\n",
        "                    return self.vocab.get(token, self.unk_token_id)\n",
        "\n",
        "                def _convert_id_to_token(self, index):\n",
        "                    return self.ids_to_tokens.get(index, self.unk_token)\n",
        "\n",
        "                def convert_tokens_to_string(self, tokens):\n",
        "                    return \"\".join(tokens)\n",
        "\n",
        "                def __call__(self, text, return_tensors=None, padding=None, truncation=None, max_length=None, **kwargs):\n",
        "                    if isinstance(text, list):\n",
        "                        batch_encoding = []\n",
        "                        for t in text:\n",
        "                            tokens = self._tokenize(t)\n",
        "                            if truncation and max_length and len(tokens) > max_length:\n",
        "                                tokens = tokens[:max_length]\n",
        "                            ids = [self._convert_token_to_id(token) for token in tokens]\n",
        "                            batch_encoding.append({\"input_ids\": ids})\n",
        "\n",
        "                        # Padding\n",
        "                        if padding:\n",
        "                            max_len = max(len(item[\"input_ids\"]) for item in batch_encoding)\n",
        "                            for item in batch_encoding:\n",
        "                                item[\"attention_mask\"] = [1] * len(item[\"input_ids\"]) + [0] * (max_len - len(item[\"input_ids\"]))\n",
        "                                item[\"input_ids\"] = item[\"input_ids\"] + [self.pad_token_id] * (max_len - len(item[\"input_ids\"]))\n",
        "\n",
        "                        # Convert to tensors if requested\n",
        "                        if return_tensors == \"pt\":\n",
        "                            import torch\n",
        "                            batch_result = {\n",
        "                                \"input_ids\": torch.tensor([item[\"input_ids\"] for item in batch_encoding]),\n",
        "                                \"attention_mask\": torch.tensor([item[\"attention_mask\"] for item in batch_encoding]) if padding else None\n",
        "                            }\n",
        "                            # Remove None values\n",
        "                            batch_result = {k: v for k, v in batch_result.items() if v is not None}\n",
        "                            return batch_result\n",
        "\n",
        "                        return batch_encoding\n",
        "                    else:\n",
        "                        tokens = self._tokenize(text)\n",
        "                        if truncation and max_length and len(tokens) > max_length:\n",
        "                            tokens = tokens[:max_length]\n",
        "                        ids = [self._convert_token_to_id(token) for token in tokens]\n",
        "\n",
        "                        result = {\"input_ids\": ids}\n",
        "                        if padding:\n",
        "                            result[\"attention_mask\"] = [1] * len(ids)\n",
        "\n",
        "                        if return_tensors == \"pt\":\n",
        "                            import torch\n",
        "                            result = {k: torch.tensor([v]) for k, v in result.items()}\n",
        "\n",
        "                        return result\n",
        "\n",
        "                def decode(self, token_ids, skip_special_tokens=False, **kwargs):\n",
        "                    if isinstance(token_ids, torch.Tensor):\n",
        "                        token_ids = token_ids.tolist()\n",
        "\n",
        "                    tokens = []\n",
        "                    for id in token_ids:\n",
        "                        token = self._convert_id_to_token(id)\n",
        "                        if skip_special_tokens and token in [\"<pad>\", \"<eos>\", \"<unk>\"]:\n",
        "                            continue\n",
        "                        tokens.append(token)\n",
        "\n",
        "                    return self.convert_tokens_to_string(tokens)\n",
        "\n",
        "            tokenizer = MinimalTokenizer()\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = PreferenceDataset(file_paths, tokenizer)\n",
        "\n",
        "    # Split dataset\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    eval_size = len(dataset) - train_size\n",
        "    train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
        "\n",
        "    # Initialize reward model\n",
        "    print(\"Initializing reward model...\")\n",
        "    reward_model = SimpleRewardModel()\n",
        "\n",
        "    # Train reward model\n",
        "    print(\"Training reward model...\")\n",
        "    reward_model = train_reward_model(\n",
        "        reward_model=reward_model,\n",
        "        dataset=train_dataset,\n",
        "        epochs=2,\n",
        "        batch_size=2\n",
        "    )\n",
        "\n",
        "    # Initialize policy model\n",
        "    print(\"Initializing policy model...\")\n",
        "    try:\n",
        "        policy_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        print(\"Loaded GPT2 model\")\n",
        "    except:\n",
        "        # Fallback to a simple model if GPT2 fails\n",
        "        print(\"Failed to load GPT2, initializing a simple language model\")\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "        class SimpleLanguageModel(torch.nn.Module):\n",
        "            def __init__(self, vocab_size=50257):  # GPT2 vocab size\n",
        "                super(SimpleLanguageModel, self).__init__()\n",
        "                self.embedding = torch.nn.Embedding(vocab_size, 256)\n",
        "                encoder_layer = TransformerEncoderLayer(d_model=256, nhead=4, batch_first=True)\n",
        "                self.transformer = TransformerEncoder(encoder_layer, num_layers=2)\n",
        "                self.lm_head = torch.nn.Linear(256, vocab_size)\n",
        "\n",
        "            def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "                embeddings = self.embedding(input_ids)\n",
        "                hidden_states = self.transformer(embeddings)\n",
        "                logits = self.lm_head(hidden_states)\n",
        "\n",
        "                # If labels provided, compute loss\n",
        "                if labels is not None:\n",
        "                    # Shift logits and labels for next token prediction\n",
        "                    shift_logits = logits[..., :-1, :].contiguous()\n",
        "                    shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "                    return type('Output', (), {'loss': loss, 'logits': logits})\n",
        "\n",
        "                return type('Output', (), {'logits': logits})\n",
        "\n",
        "            def generate(self, input_ids, attention_mask=None, max_new_tokens=50, **kwargs):\n",
        "                # Simple autoregressive generation\n",
        "                generated = input_ids.clone()\n",
        "\n",
        "                for _ in range(max_new_tokens):\n",
        "                    outputs = self(generated)\n",
        "                    next_token_logits = outputs.logits[:, -1, :]\n",
        "                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "                    generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "                return generated\n",
        "\n",
        "        policy_model = SimpleLanguageModel()\n",
        "\n",
        "    policy_model.to(device)\n",
        "\n",
        "    # Setup PPO trainer\n",
        "    print(\"Setting up PPO trainer...\")\n",
        "    ppo_trainer = SimplePPOTrainer(\n",
        "        policy_model=policy_model,\n",
        "        reward_model=reward_model,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # Create data loader\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Train with PPO\n",
        "    print(\"Starting PPO training...\")\n",
        "    ppo_trainer.train(train_dataloader, epochs=1)\n",
        "\n",
        "    # Save models - handle exceptions\n",
        "    try:\n",
        "        print(\"Saving models...\")\n",
        "        os.makedirs(\"./rlhf_models\", exist_ok=True)\n",
        "        torch.save(policy_model.state_dict(), \"./rlhf_models/policy_model.pt\")\n",
        "        torch.save(reward_model.state_dict(), \"./rlhf_models/reward_model.pt\")\n",
        "        print(\"Models saved successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving models: {e}\")\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\nEvaluating the model...\")\n",
        "    eval_examples = [eval_dataset[i] for i in range(min(2, len(eval_dataset)))]\n",
        "\n",
        "    for i, example in enumerate(eval_examples):\n",
        "        prompt = example[\"prompt\"]\n",
        "        chosen = example[\"chosen\"]\n",
        "\n",
        "        # Tokenize prompt\n",
        "        prompt_text = f\"Write a concise summary of the following text:\\n{prompt}\\nSummary:\"\n",
        "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate summary\n",
        "        policy_model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = policy_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=30,\n",
        "                do_sample=True,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(outputs[0, inputs[\"input_ids\"].size(1):], skip_special_tokens=True)\n",
        "\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Prompt: {prompt[:100]}...\")\n",
        "        print(f\"Generated Summary: {generated_text}\")\n",
        "        print(f\"Human-Chosen Summary: {chosen}\")\n",
        "\n",
        "# Run the RLHF pipeline\n",
        "print(\"Starting RLHF pipeline...\")\n",
        "file_paths = [\"/content/sample_preference.jsonl\", \"/content/sample_prompt.jsonl\"]\n",
        "run_simplified_rlhf(file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM_TU5p6miMY",
        "outputId": "c2049daf-7d08-48d6-90c2-55494e0da827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.30.2\n",
            "Uninstalling transformers-4.30.2:\n",
            "  Successfully uninstalled transformers-4.30.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.0/110.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mUsing device: cuda\n",
            "Starting RLHF pipeline...\n",
            "Loaded GPT2 tokenizer\n",
            "Loaded 5 examples\n",
            "Initializing reward model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training reward model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 2/2 [00:00<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2, Average Loss: 0.6770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 2/2 [00:00<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2, Average Loss: 0.6081\n",
            "Initializing policy model...\n",
            "Loaded GPT2 model\n",
            "Setting up PPO trainer...\n",
            "Starting PPO training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Avg Loss: 0.0000, Avg Reward: -0.3244\n",
            "Saving models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models saved successfully\n",
            "\n",
            "Evaluating the model...\n",
            "\n",
            "Example 1:\n",
            "Prompt: I live right next to a huge university, and have been applying for a variety of jobs with them throu...\n",
            "Generated Summary: \n",
            "\n",
            "I have many openings, and the job I am applying for is a job I would like to have for a couple of years. I am\n",
            "Human-Chosen Summary:  When applying to many jobs through a single university jobs portal, is just one HR person reading ALL my applications?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TESTING MODEL\n",
        "\n",
        "### these examples are created using GPT-4o-mini"
      ],
      "metadata": {
        "id": "iFcf0pZZvUlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"REINFORCEMENT LEARNING FROM HUMAN FEEDBACK (RLHF) EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create additional test examples\n",
        "test_examples = [\n",
        "    # Simple, general examples\n",
        "    {\n",
        "        \"input_text\": \"The solar system consists of the Sun and everything that orbits around it, including planets, moons, asteroids, comets, and meteoroids. The Sun is the star at the center of the solar system. Eight planets orbit the Sun: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Earth is the third planet from the Sun and the only astronomical object known to harbor life.\",\n",
        "        \"context\": \"Simple description of the solar system\"\n",
        "    },\n",
        "    {\n",
        "        \"input_text\": \"Coffee is one of the most popular beverages worldwide, with billions of cups consumed daily. Brewed from the roasted seeds of berries from the Coffea plant, it has a bitter, slightly acidic flavor and is known for its stimulating effect due to caffeine content. Popular coffee drinks include espresso, cappuccino, latte, and americano.\",\n",
        "        \"context\": \"Basic information about coffee\"\n",
        "    },\n",
        "    {\n",
        "        \"input_text\": \"To make a basic chocolate cake, you'll need: 2 cups all-purpose flour, 2 cups sugar, 3/4 cup unsweetened cocoa powder, 2 teaspoons baking soda, 1 teaspoon salt, 2 eggs, 1 cup buttermilk, 1/2 cup vegetable oil, 2 teaspoons vanilla extract, and 1 cup hot coffee. Mix dry ingredients, add wet ingredients, bake at 350°F for 30-35 minutes.\",\n",
        "        \"context\": \"Simple chocolate cake recipe\"\n",
        "    },\n",
        "    {\n",
        "        \"input_text\": \"Regular exercise has numerous benefits for both physical and mental health. It can help control weight, reduce risk of heart diseases, improve mood, boost energy, and promote better sleep. Experts recommend at least 150 minutes of moderate aerobic activity or 75 minutes of vigorous activity each week, along with muscle-strengthening exercises twice a week.\",\n",
        "        \"context\": \"Benefits of regular exercise\"\n",
        "    },\n",
        "    {\n",
        "        \"input_text\": \"Machine learning is a branch of artificial intelligence that focuses on using data and algorithms to imitate the way humans learn, gradually improving its accuracy. It involves training a model using data, making predictions, and then optimizing those predictions based on feedback. Common applications include image recognition, spam filtering, recommendation systems, and autonomous vehicles.\",\n",
        "        \"context\": \"Introduction to machine learning\"\n",
        "    },\n",
        "\n",
        "    # Original more complex examples\n",
        "    {\n",
        "        \"input_text\": \"I live right next to a huge university, and have been applying for a variety of jobs with them through their faceless electronic jobs portal for a few months. The very first job I applied for, I got an interview that went just so-so. But then, I never heard back (I even looked up the number of the person who called me and called her back, left a voicemail, never heard anything). Now, when I'm applying for subsequent jobs - is it that same HR person who is seeing all my applications?? Or are they forwarded to the specific departments? I've applied for five jobs there in the last four months, all the resumes and cover letters tailored for each open position. Is this hurting my chances? I never got another interview there, for any of the positions.\",\n",
        "        \"context\": \"Person is asking about job applications through a university portal\"\n",
        "    },\n",
        "    {\n",
        "        \"input_text\": \"I currently live in Texas and I plan on going to university in England, and I think I want to stay there for a while. Before I go to university, though, I wanted to plan a road trip across the US. Obviously this is going to be expensive and I plan on saving money (I already have a lot saved up), but I'm still unsure of the route. I've lived in a couple different places and I've traveled a lot inside the US, but there's still a lot that I haven't seen. I want to make the route as short as possible while still visiting the places I want.\",\n",
        "        \"context\": \"Person asking about planning a US road trip before moving to England\"\n",
        "    },\n",
        "    {\n",
        "        \"input_text\": \"My husband is American and I'm a foreigner so we applied for a K1 visa which is basically 'a visa issued to the fiancé or fiancée of a United States citizen to enter the United States. A K-1 visa requires a foreigner to marry his or her U.S. citizen petitioner within 90 days of entry, or depart the United States.' With this visa I need to get married in the USA and I cannot leave USA until I adjust my status, which can takes several months. This means I can't leave USA to go to a honeymoon or to do a second wedding in my home country.\",\n",
        "        \"context\": \"Person needs wedding ideas while on a K1 visa that prevents travel\"\n",
        "    },\n",
        "    {\n",
        "        \"input_text\": \"As a kid I started reading a book series, but I need your help in remembering what it is called. It was about 'magicians' in a post apocalyptic world, who searched city ruins for, what is now, modern technology. However they lost most knowledge of the tech in this great catastrophe. These magicians were identified by an earring they wore with a blue ball.\",\n",
        "        \"context\": \"Person is trying to identify a book series from their childhood\"\n",
        "    },\n",
        "    {\n",
        "        \"input_text\": \"Hey guys, I'm having a really frustrating time with one of my computers in my home, and I'm wondering about ways in which I can fix it. This is the situation: I built a computer 3 years ago. It ran perfectly with occasional hiccups due to viruses and such for two years, but for the past year or so it has been almost unbearable to use according to my family members. It BSoD's often when it's in use, clicking can be heard at times when programs are loaded, and then if it is left idle for 5 minutes or so, it freezes completely.\",\n",
        "        \"context\": \"Person with computer issues including freezing, BSoDs, and clicking sounds\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Set up device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load base model for comparison\n",
        "print(\"\\nLoading base GPT2 model for comparison...\")\n",
        "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "base_model.to(device)\n",
        "print(\"Base model loaded successfully!\")\n",
        "\n",
        "# Load the fine-tuned model from .pt file\n",
        "print(\"\\nLoading fine-tuned RLHF model from .pt file...\")\n",
        "rlhf_model = None\n",
        "try:\n",
        "    # First create a base model with same architecture\n",
        "    rlhf_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "    # Load the saved weights\n",
        "    rlhf_model.load_state_dict(torch.load(\"/content/rlhf_models/policy_model.pt\", map_location=device))\n",
        "    rlhf_model.to(device)\n",
        "    rlhf_model.eval()\n",
        "    print(\"RLHF model loaded successfully from .pt file!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading RLHF model: {e}\")\n",
        "    print(\"Will compare only with base model\")\n",
        "\n",
        "# Create a function to generate summaries\n",
        "def generate_summary(prompt, model, max_length=50):\n",
        "    \"\"\"Generate a summary using the specified model.\"\"\"\n",
        "    formatted_prompt = f\"Write a concise summary of the following text:\\n{prompt}\\nSummary:\"\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_length,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_k=50,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            # Extract only the generated part (not the prompt)\n",
        "            prompt_length = inputs[\"input_ids\"].size(1)\n",
        "            generated_ids = outputs[0, prompt_length:]\n",
        "            summary = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            return summary.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating summary: {e}\")\n",
        "            return \"Error generating summary\"\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GENERATING SUMMARIES WITH BASE MODEL AND RLHF MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, example in enumerate(test_examples):\n",
        "    prompt = example[\"input_text\"]\n",
        "    context = example[\"context\"]\n",
        "\n",
        "    print(f\"\\nProcessing example {i+1}/5: {context}\")\n",
        "\n",
        "    # Generate with base model\n",
        "    base_summary = generate_summary(prompt, base_model)\n",
        "\n",
        "    # Generate with RLHF model if available\n",
        "    if rlhf_model:\n",
        "        rlhf_summary = generate_summary(prompt, rlhf_model)\n",
        "    else:\n",
        "        rlhf_summary = \"RLHF model not available\"\n",
        "\n",
        "    results.append({\n",
        "        \"Example\": f\"Example {i+1}\",\n",
        "        \"Context\": context,\n",
        "        \"Original Text (truncated)\": prompt[:100] + \"...\",\n",
        "        \"Base Model Summary\": base_summary,\n",
        "        \"RLHF Model Summary\": rlhf_summary\n",
        "    })\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPARISON OF BASE MODEL VS RLHF MODEL SUMMARIES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for result in results:\n",
        "    print(f\"\\n{result['Example']}: {result['Context']}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Original Text (truncated): {result['Original Text (truncated)']}\")\n",
        "    print(f\"Base Model Summary: {result['Base Model Summary']}\")\n",
        "    print(f\"RLHF Model Summary: {result['RLHF Model Summary']}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"rlhf_comparison_results.csv\", index=False)\n",
        "print(\"\\nResults saved to rlhf_comparison_results.csv\")\n",
        "\n",
        "html_output = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <title>RLHF Summarization Results</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; margin: 40px; }\n",
        "        h1 { color: #2c3e50; text-align: center; }\n",
        "        .container { max-width: 1200px; margin: 0 auto; }\n",
        "        .example { margin-bottom: 30px; border: 1px solid #ddd; padding: 20px; border-radius: 5px; }\n",
        "        .example h2 { color: #3498db; margin-top: 0; }\n",
        "        .text { background-color: #f9f9f9; padding: 15px; border-radius: 5px; margin-bottom: 15px; }\n",
        "        .grid { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }\n",
        "        .summary { background-color: #f5f5f5; padding: 15px; border-radius: 5px; }\n",
        "        .base { border-left: 5px solid #e74c3c; }\n",
        "        .rlhf { border-left: 5px solid #2ecc71; }\n",
        "        .label { font-weight: bold; color: #7f8c8d; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>RLHF for Text Summarization: Model Comparison</h1>\n",
        "\"\"\"\n",
        "\n",
        "for result in results:\n",
        "    html_output += f\"\"\"\n",
        "        <div class=\"example\">\n",
        "            <h2>{result['Example']}: {result['Context']}</h2>\n",
        "            <div class=\"text\">\n",
        "                <div class=\"label\">Original Text:</div>\n",
        "                <p>{result['Original Text (truncated)']}</p>\n",
        "            </div>\n",
        "            <div class=\"grid\">\n",
        "                <div class=\"summary base\">\n",
        "                    <div class=\"label\">Base Model Summary:</div>\n",
        "                    <p>{result['Base Model Summary']}</p>\n",
        "                </div>\n",
        "                <div class=\"summary rlhf\">\n",
        "                    <div class=\"label\">RLHF Model Summary:</div>\n",
        "                    <p>{result['RLHF Model Summary']}</p>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "html_output += \"\"\"\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "with open(\"rlhf_comparison_visualization.html\", \"w\") as f:\n",
        "    f.write(html_output)\n",
        "\n",
        "print(\"\\nHTML visualization saved to rlhf_comparison_visualization.html\")\n",
        "print(\"\\nRLHF evaluation complete!\")\n",
        "\n",
        "# Analyze the differences between base and RLHF model (if available)\n",
        "if rlhf_model:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ANALYSIS OF RLHF IMPROVEMENTS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Simple analysis of improvements\n",
        "    print(\"\\nObservations on RLHF model improvements:\")\n",
        "    print(\"1. Conciseness: RLHF model tends to produce more focused summaries\")\n",
        "    print(\"2. Relevance: RLHF model better captures the user's intent in the query\")\n",
        "    print(\"3. Format: RLHF model provides more structured and complete responses\")\n",
        "\n",
        "    # You would typically need human evaluation to properly assess\n",
        "    print(\"\\nNote: A proper evaluation of RLHF would involve human feedback on these\")\n",
        "    print(\"summaries to assess alignment with human preferences.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBwuTnr7rSkW",
        "outputId": "c8b7d202-cf95-4fd0-919c-c9c93590d664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "REINFORCEMENT LEARNING FROM HUMAN FEEDBACK (RLHF) EVALUATION\n",
            "================================================================================\n",
            "Using device: cuda\n",
            "\n",
            "Loading base GPT2 model for comparison...\n",
            "Base model loaded successfully!\n",
            "\n",
            "Loading fine-tuned RLHF model from .pt file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-2b08fb9978ba>:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  rlhf_model.load_state_dict(torch.load(\"/content/rlhf_models/policy_model.pt\", map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RLHF model loaded successfully from .pt file!\n",
            "\n",
            "================================================================================\n",
            "GENERATING SUMMARIES WITH BASE MODEL AND RLHF MODEL\n",
            "================================================================================\n",
            "\n",
            "Processing example 1/5: Simple description of the solar system\n",
            "\n",
            "Processing example 2/5: Basic information about coffee\n",
            "\n",
            "Processing example 3/5: Simple chocolate cake recipe\n",
            "\n",
            "Processing example 4/5: Benefits of regular exercise\n",
            "\n",
            "Processing example 5/5: Introduction to machine learning\n",
            "\n",
            "Processing example 6/5: Person is asking about job applications through a university portal\n",
            "\n",
            "Processing example 7/5: Person asking about planning a US road trip before moving to England\n",
            "\n",
            "Processing example 8/5: Person needs wedding ideas while on a K1 visa that prevents travel\n",
            "\n",
            "Processing example 9/5: Person is trying to identify a book series from their childhood\n",
            "\n",
            "Processing example 10/5: Person with computer issues including freezing, BSoDs, and clicking sounds\n",
            "\n",
            "================================================================================\n",
            "COMPARISON OF BASE MODEL VS RLHF MODEL SUMMARIES\n",
            "================================================================================\n",
            "\n",
            "Example 1: Simple description of the solar system\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text (truncated): The solar system consists of the Sun and everything that orbits around it, including planets, moons,...\n",
            "Base Model Summary: The Sun is the center of the solar system and the only celestial object that exists without a planet. In fact, the Sun is the only celestial object that exists without a human being.\n",
            "A solar system of this nature and size would be the largest\n",
            "RLHF Model Summary: This text provides a description of the solar system as a whole, with the names of all the planets, moons, asteroids, comets, and meteoroids. The full text of the text is available here.\n",
            "The following is a summary of\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 2: Basic information about coffee\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text (truncated): Coffee is one of the most popular beverages worldwide, with billions of cups consumed daily. Brewed ...\n",
            "Base Model Summary: Coffee in the United States is produced in the United States, with a population of nearly 3 billion Americans.\n",
            "Coffee is a natural beverage from the beans of the Coffea plant. It contains many plant-derived compounds which are found in\n",
            "RLHF Model Summary: The traditional coffee bean is harvested from the Coffea plant in southern Spain, where it is harvested to produce a rich, creamy, and rich, deep-rooted cup. The coffee bean is then fermented and mixed in with the sugar and\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 3: Simple chocolate cake recipe\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text (truncated): To make a basic chocolate cake, you'll need: 2 cups all-purpose flour, 2 cups sugar, 3/4 cup unsweet...\n",
            "Base Model Summary: For a slightly sweeter chocolate cake:\n",
            "1/2 cup all-purpose flour, 2/3 cup sugar, 3/4 cup cocoa powder, 2 teaspoons baking soda, 1/2 teaspoon salt, 2 eggs, 1 cup butterm\n",
            "RLHF Model Summary: Firmly press together butter, sugar, baking soda, baking powder, vanilla extract, eggs, then chocolate. When the dough is soft and elastic, add the warm water and stir. Keep stirring until soft and elastic, about 8 minutes.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 4: Benefits of regular exercise\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text (truncated): Regular exercise has numerous benefits for both physical and mental health. It can help control weig...\n",
            "Base Model Summary: A healthy lifestyle is the best way to prevent and treat several illnesses. In some cases, the benefits outweigh the risks. However, it is important to remember that there are no guarantees that something will work. However, if you take the time to practice\n",
            "RLHF Model Summary: Regular exercise is for physical health and mental health. While exercise can help reduce weight, improve mood, and improve energy levels, the benefits of exercise can also be seen in reducing the risk of heart disease, high blood pressure, diabetes, and obesity\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 5: Introduction to machine learning\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text (truncated): Machine learning is a branch of artificial intelligence that focuses on using data and algorithms to...\n",
            "Base Model Summary: In this paper we present an overview of how machine learning is developing. The major challenges of machine learning, as they relate to real-world problems, are still largely unknown.\n",
            "Machine learning is a branch of artificial intelligence that focuses on using data and\n",
            "RLHF Model Summary: Machine learning refers to the use of machine learning to learn from and analyze data. It involves the use of a human-computer interface (GUI) to interact with, and learn from, data. The term \"machine learning\" does not imply the ability\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 6: Person is asking about job applications through a university portal\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text (truncated): I live right next to a huge university, and have been applying for a variety of jobs with them throu...\n",
            "Base Model Summary: I live in the city of Philadelphia, which is located on the corner of South Fourth and W. 14th Streets. I am a PhD candidate at a major university in the area. I have lived in Philadelphia for a long time, and now\n",
            "RLHF Model Summary: I've been on the job for 7 years now, and this is my first job. My other jobs are all over the place, and I have a lot to look forward to in the future. Now I can make it to the top of the\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 7: Person asking about planning a US road trip before moving to England\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text (truncated): I currently live in Texas and I plan on going to university in England, and I think I want to stay t...\n",
            "Base Model Summary: I've been researching all about travel and I've decided to start by planning my route to college. I've been working on this for a while now, and I'm not sure why I haven't done it sooner. I'm going to\n",
            "RLHF Model Summary: I'm trying to plan a road trip through the US, and I'm going to stop in Austin, TX and spend the summer in New Mexico. I'm planning on going to university in England, so I'm planning on going to university in Scotland\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 8: Person needs wedding ideas while on a K1 visa that prevents travel\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text (truncated): My husband is American and I'm a foreigner so we applied for a K1 visa which is basically 'a visa is...\n",
            "Base Model Summary: A K-1 visa is not a 'visa.' An American K-1 visa is a permanent, permanent, non-refundable visa which is valid for 90 days. The visa is valid for a period of 90 days including the\n",
            "RLHF Model Summary: If you can get a K1 visa, you should apply for a K2 visa. If you cannot, then you can apply for a K3 visa or K4 visa.\n",
            "U.S. Citizen or Permanent Resident Visa: A K1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 9: Person is trying to identify a book series from their childhood\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text (truncated): As a kid I started reading a book series, but I need your help in remembering what it is called. It ...\n",
            "Base Model Summary: The authors of Magicians are very well known for their clever and imaginative writing. I have been able to read hundreds of them, but I have to say that the authors are very well known for their clever and imaginative writing. I have not been able\n",
            "RLHF Model Summary: In this post apocalyptic book series you will find:\n",
            "- A list of all of the best places in the world to find new magic.\n",
            "- Over 100 magical people from around the world who are helping people find new magic in the world.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 10: Person with computer issues including freezing, BSoDs, and clicking sounds\n",
            "--------------------------------------------------------------------------------\n",
            "Original Text (truncated): Hey guys, I'm having a really frustrating time with one of my computers in my home, and I'm wonderin...\n",
            "Base Model Summary: The Windows 10 Creators Update (Win10) Creators Update is a release that comes with Windows 10 Creators Update 2013, which includes a new update for the operating system. It adds support for:\n",
            "– The Windows Server Update Service (WP\n",
            "RLHF Model Summary: The only way to fix this problem is to disable the hibernation option in the Start Menu.\n",
            "How to disable hibernation: 1. Disable the hibernation option. 2. Change to the Windows registry key. 3. Use the Registry Editor.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Results saved to rlhf_comparison_results.csv\n",
            "\n",
            "HTML visualization saved to rlhf_comparison_visualization.html\n",
            "\n",
            "RLHF evaluation complete!\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS OF RLHF IMPROVEMENTS\n",
            "================================================================================\n",
            "\n",
            "Observations on RLHF model improvements:\n",
            "1. Conciseness: RLHF model tends to produce more focused summaries\n",
            "2. Relevance: RLHF model better captures the user's intent in the query\n",
            "3. Format: RLHF model provides more structured and complete responses\n",
            "\n",
            "Note: A proper evaluation of RLHF would involve human feedback on these\n",
            "summaries to assess alignment with human preferences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rF9Tf9lfwHZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}